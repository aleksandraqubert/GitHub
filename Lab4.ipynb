{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2BtsMymHtqPTCgufCcuKf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aleksandraqubert/GitHub/blob/main/Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BR1SrF793rvO",
        "outputId": "2d0b63af-0bdc-4317-9913-cb17145f563c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=2bed05f234445302407823350807dc06ac711cef6bec79aa54e7907d0a99b5be\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Конфігурація\n",
        "\n",
        "- `number_cores`: Кількість ядер, виділених під Spark\n",
        "- `memory_gb`: Обʼєм оперативної памʼяті, виділеної під Spark (в Гб)"
      ],
      "metadata": {
        "id": "U03_28bB7nmN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NQrsOjERwBMa"
      },
      "outputs": [],
      "source": [
        "number_cores = 2\n",
        "memory_gb = 4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "spark = (SparkSession\n",
        "    .builder\n",
        "    .appName('Spark Bikes')\n",
        "    .master(f\"local[{number_cores}]\")\n",
        "    .config(\"spark.driver.memory\", f\"{memory_gb}g\")\n",
        "    .getOrCreate())"
      ],
      "metadata": {
        "id": "l6fCQ9ve3e9A"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Рішення\n",
        "Прочитайте вміст вхідного файлу `register.csv` і збережіть його у DataFrame.\n",
        "\n",
        "Вхідний файл має заголовок.\n",
        "\n",
        "Схема даних:\n",
        "* station: integer (nullable = true)\n",
        "* timestamp: timestamp (nullable = true)\n",
        "* used_slots: integer (nullable = true)\n",
        "* free_slots: integer (nullable = true)"
      ],
      "metadata": {
        "id": "0fiM4n897jqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Ініціалізуємо SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Read Register CSV\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Читаємо вміст вхідного файлу register.csv та зберігаємо його у DataFrame\n",
        "register_df = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .option(\"delimiter\", \"\\t\") \\\n",
        "    .load(\"register.csv\")\n",
        "\n",
        "# Виводимо  для перевірки\n",
        "register_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_x2pf4X3fMt",
        "outputId": "5910db43-77e9-41ce-b8d2-803575d1702c"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+----------+----------+\n",
            "|station|          timestamp|used_slots|free_slots|\n",
            "+-------+-------------------+----------+----------+\n",
            "|      1|2023-05-15 12:01:00|         0|        18|\n",
            "|      1|2023-05-15 12:02:00|         0|        18|\n",
            "|      1|2023-05-15 12:04:00|         0|        18|\n",
            "|      1|2023-05-15 12:06:00|         0|        18|\n",
            "|      1|2023-05-15 12:08:00|         0|        18|\n",
            "|      1|2023-05-15 12:10:00|         0|        18|\n",
            "|      1|2023-05-15 12:12:00|         0|        18|\n",
            "|      1|2023-05-15 12:14:00|         0|        18|\n",
            "|      1|2023-05-15 12:16:00|         0|        18|\n",
            "|      1|2023-05-15 12:18:00|         0|        18|\n",
            "|      1|2023-05-15 12:20:00|         2|        16|\n",
            "|      1|2023-05-15 12:22:00|         3|        15|\n",
            "|      1|2023-05-15 12:24:00|         3|        15|\n",
            "|      1|2023-05-15 12:26:00|         3|        15|\n",
            "|      1|2023-05-15 12:28:00|         4|        14|\n",
            "|      1|2023-05-15 12:30:00|         0|        12|\n",
            "|      1|2023-05-15 12:32:00|         4|        14|\n",
            "|      1|2023-05-15 12:34:00|         4|        14|\n",
            "|      1|2023-05-15 12:36:00|         4|        14|\n",
            "|      1|2023-05-15 12:38:00|         4|        14|\n",
            "+-------+-------------------+----------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Видаліть рядки де одночасно `free_slots = 0` та `used_slots = 0`"
      ],
      "metadata": {
        "id": "6H00XDQJ7d0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n",
        "# Фільтруємо рядки, де free_slots не дорівнює 0 або used_slots не дорівнює 0\n",
        "filtered_register_df = register_df.filter((register_df[\"free_slots\"] != 0) & (register_df[\"used_slots\"] != 0))\n",
        "\n",
        "# Виводимо для перевірки\n",
        "\n",
        "filtered_register_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwgfcUTL3fUu",
        "outputId": "7efda20f-71db-4876-e467-329e4978b13b"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+----------+----------+-------+----------+--------+----------+\n",
            "|station|          timestamp|used_slots|free_slots|is_full| dayofweek|    hour|fullstatus|\n",
            "+-------+-------------------+----------+----------+-------+----------+--------+----------+\n",
            "|      1|2023-05-15 12:20:00|         2|        16|      0|2023-05-15|12:20:00|         0|\n",
            "|      1|2023-05-15 12:22:00|         3|        15|      0|2023-05-15|12:22:00|         0|\n",
            "|      1|2023-05-15 12:24:00|         3|        15|      0|2023-05-15|12:24:00|         0|\n",
            "|      1|2023-05-15 12:26:00|         3|        15|      0|2023-05-15|12:26:00|         0|\n",
            "|      1|2023-05-15 12:28:00|         4|        14|      0|2023-05-15|12:28:00|         0|\n",
            "|      1|2023-05-15 12:32:00|         4|        14|      0|2023-05-15|12:32:00|         0|\n",
            "|      1|2023-05-15 12:34:00|         4|        14|      0|2023-05-15|12:34:00|         0|\n",
            "|      1|2023-05-15 12:36:00|         4|        14|      0|2023-05-15|12:36:00|         0|\n",
            "|      1|2023-05-15 12:38:00|         4|        14|      0|2023-05-15|12:38:00|         0|\n",
            "|      1|2023-05-15 12:40:00|         5|        13|      0|2023-05-15|12:40:00|         0|\n",
            "|      1|2023-05-15 12:42:00|         6|        12|      0|2023-05-15|12:42:00|         0|\n",
            "|      1|2023-05-15 12:46:00|         6|        12|      0|2023-05-15|12:46:00|         0|\n",
            "|      1|2023-05-15 12:48:00|         7|        11|      0|2023-05-15|12:48:00|         0|\n",
            "|      1|2023-05-15 12:50:00|         7|        11|      0|2023-05-15|12:50:00|         0|\n",
            "|      1|2023-05-15 12:52:00|         7|        11|      0|2023-05-15|12:52:00|         0|\n",
            "|      1|2023-05-15 12:54:00|         8|        10|      0|2023-05-15|12:54:00|         0|\n",
            "|      1|2023-05-15 12:56:00|         9|         9|      0|2023-05-15|12:56:00|         0|\n",
            "|      1|2023-05-15 12:58:00|         9|         9|      0|2023-05-15|12:58:00|         0|\n",
            "|      1|2023-05-15 13:00:00|         9|         9|      0|2023-05-15|13:00:00|         0|\n",
            "|      1|2023-05-15 13:02:00|         9|         9|      0|2023-05-15|13:02:00|         0|\n",
            "+-------+-------------------+----------+----------+-------+----------+--------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Нам потрібен логічний маркер, щоб побачити, заповнена станція чи ні. Це можна зробити за допомогою UDF під назвою `full(free_slots: int)`, яка повертає\n",
        "* 1, якщо `free_slots` дорівнює 0\n",
        "* 0, якщо `free_slots` більше 0\n",
        "\n",
        "> Якщо ви використовуєте Pandas on Spark API, то треба самостійно застосувати цю функцію (або переписати її)"
      ],
      "metadata": {
        "id": "5xf7UZH37Zc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Визначаємо функцію UDF\n",
        "def full_function(free_slots: int) -> int:\n",
        "    if free_slots == 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Реєструємо функцію UDF\n",
        "full_udf = udf(full_function, IntegerType())\n",
        "\n",
        "# Додаємо новий стовпець, який використовує функцію UDF\n",
        "register_df = register_df.withColumn(\"is_full\", full_udf(register_df[\"free_slots\"]))\n",
        "\n",
        "# Виводимо схему даних та  рядки для перевірки\n",
        "register_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC5jv5hA3faO",
        "outputId": "eb869253-1f3b-41e5-8f7a-4514760f895e"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+----------+----------+-------+\n",
            "|station|          timestamp|used_slots|free_slots|is_full|\n",
            "+-------+-------------------+----------+----------+-------+\n",
            "|      1|2023-05-15 12:01:00|         0|        18|      0|\n",
            "|      1|2023-05-15 12:02:00|         0|        18|      0|\n",
            "|      1|2023-05-15 12:04:00|         0|        18|      0|\n",
            "|      1|2023-05-15 12:06:00|         0|        18|      0|\n",
            "|      1|2023-05-15 12:08:00|         0|        18|      0|\n",
            "|      1|2023-05-15 12:10:00|         0|        18|      0|\n",
            "|      1|2023-05-15 12:12:00|         0|        18|      0|\n",
            "|      1|2023-05-15 12:14:00|         0|        18|      0|\n",
            "|      1|2023-05-15 12:16:00|         0|        18|      0|\n",
            "|      1|2023-05-15 12:18:00|         0|        18|      0|\n",
            "|      1|2023-05-15 12:20:00|         2|        16|      0|\n",
            "|      1|2023-05-15 12:22:00|         3|        15|      0|\n",
            "|      1|2023-05-15 12:24:00|         3|        15|      0|\n",
            "|      1|2023-05-15 12:26:00|         3|        15|      0|\n",
            "|      1|2023-05-15 12:28:00|         4|        14|      0|\n",
            "|      1|2023-05-15 12:30:00|         0|        12|      0|\n",
            "|      1|2023-05-15 12:32:00|         4|        14|      0|\n",
            "|      1|2023-05-15 12:34:00|         4|        14|      0|\n",
            "|      1|2023-05-15 12:36:00|         4|        14|      0|\n",
            "|      1|2023-05-15 12:38:00|         4|        14|      0|\n",
            "+-------+-------------------+----------+----------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Створіть DataFrame з такою схемою:\n",
        "* station: integer (nullable = true)\n",
        "* dayofweek: string (nullable = true)\n",
        "* hour: integer (nullable = true)\n",
        "* fullstatus: integer (nullable = true) - 1 = full, 0 = non-full"
      ],
      "metadata": {
        "id": "_LHLVx6V8E8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_date, hour,date_format, when\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "# Описуємо схему для DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"station\", IntegerType(), nullable=True),\n",
        "    StructField(\"timestamp\", StringType(), nullable=True)\n",
        "])\n",
        "\n",
        "\n",
        "# Розділяємо стовбець timestamp на dayofweek та hour\n",
        "register_df = register_df.withColumn(\"dayofweek\", to_date(col(\"timestamp\"))) \\\n",
        "    .withColumn(\"hour\", date_format(col(\"timestamp\"), \"HH:mm:ss\")) \\\n",
        "    .withColumn(\"fullstatus\", col(\"is_full\"))\n",
        "\n",
        "\n",
        "\n",
        "# Вибираємо тільки необхідні колонки: station, dayofweek, hour, fullstatus\n",
        "selected_df = register_df.select(\"station\", \"dayofweek\", \"hour\", \"fullstatus\")\n",
        "\n",
        "# Виведення схеми створеного DataFrame\n",
        "selected_df.printSchema()\n",
        "\n",
        "# Виведення вмісту DataFrame\n",
        "selected_df.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpcrE95S3fdO",
        "outputId": "cb45ce28-27aa-4ea9-ff2d-2f2e8959e7ca"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- station: integer (nullable = true)\n",
            " |-- dayofweek: date (nullable = true)\n",
            " |-- hour: string (nullable = true)\n",
            " |-- fullstatus: integer (nullable = true)\n",
            "\n",
            "+-------+----------+--------+----------+\n",
            "|station| dayofweek|    hour|fullstatus|\n",
            "+-------+----------+--------+----------+\n",
            "|      1|2023-05-15|12:01:00|         0|\n",
            "|      1|2023-05-15|12:02:00|         0|\n",
            "|      1|2023-05-15|12:04:00|         0|\n",
            "|      1|2023-05-15|12:06:00|         0|\n",
            "|      1|2023-05-15|12:08:00|         0|\n",
            "|      1|2023-05-15|12:10:00|         0|\n",
            "|      1|2023-05-15|12:12:00|         0|\n",
            "|      1|2023-05-15|12:14:00|         0|\n",
            "|      1|2023-05-15|12:16:00|         0|\n",
            "|      1|2023-05-15|12:18:00|         0|\n",
            "|      1|2023-05-15|12:20:00|         0|\n",
            "|      1|2023-05-15|12:22:00|         0|\n",
            "|      1|2023-05-15|12:24:00|         0|\n",
            "|      1|2023-05-15|12:26:00|         0|\n",
            "|      1|2023-05-15|12:28:00|         0|\n",
            "|      1|2023-05-15|12:30:00|         0|\n",
            "|      1|2023-05-15|12:32:00|         0|\n",
            "|      1|2023-05-15|12:34:00|         0|\n",
            "|      1|2023-05-15|12:36:00|         0|\n",
            "|      1|2023-05-15|12:38:00|         0|\n",
            "+-------+----------+--------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nHOxH3gKVY3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Визначте одну групу для кожної комбінації `(station, dayofweek, hour)`"
      ],
      "metadata": {
        "id": "HQaiJ8BZCTEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Імпорт необхідних функцій\n",
        "from pyspark.sql.functions import count\n",
        "\n",
        "# Групуємо дані за комбінаціями (station, dayofweek, hour) та рахуємо кількість рядків у кожній групі\n",
        "grouped_df = register_df.groupBy(\"station\", \"dayofweek\", \"hour\").agg(count(\"*\").alias(\"group_count\"))\n",
        "\n",
        "# Виводимо результат\n",
        "grouped_df.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NmWtK6l3fhe",
        "outputId": "eec2ed40-9413-4aaf-ba98-ef051943b937"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+--------+-----------+\n",
            "|station| dayofweek|    hour|group_count|\n",
            "+-------+----------+--------+-----------+\n",
            "|      1|2023-05-15|18:46:00|          1|\n",
            "|      1|2023-05-15|23:18:00|          1|\n",
            "|      1|2023-05-17|08:46:00|          1|\n",
            "|      1|2023-05-17|11:02:00|          1|\n",
            "|      1|2023-05-17|13:16:00|          1|\n",
            "|      1|2023-05-19|06:54:00|          1|\n",
            "|      1|2023-05-19|08:00:00|          1|\n",
            "|      1|2023-05-21|12:02:00|          1|\n",
            "|      1|2023-05-21|19:46:00|          1|\n",
            "|      1|2023-05-21|21:22:00|          1|\n",
            "|      1|2023-05-22|06:02:00|          1|\n",
            "|      1|2023-05-22|23:10:00|          1|\n",
            "|      1|2023-05-23|06:08:00|          1|\n",
            "|      1|2023-05-23|13:58:00|          1|\n",
            "|      1|2023-05-23|19:46:00|          1|\n",
            "|      1|2023-05-24|17:18:00|          1|\n",
            "|      1|2023-05-24|20:36:00|          1|\n",
            "|      1|2023-05-25|05:58:00|          1|\n",
            "|      1|2023-05-25|15:58:00|          1|\n",
            "|      1|2023-05-25|17:48:00|          1|\n",
            "+-------+----------+--------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обчисліть \"критичність\" для кожної групи `(station, dayofweek, hour)`, тобто для кожної пари `(station, timeslot)`.\n",
        "\n",
        "Критичність дорівнює середньому `fullStatus`."
      ],
      "metadata": {
        "id": "xTxVCWh5V48I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Згрупуємо дані за station, dayofweek та hour та обчислимо середнє значення fullstatus\n",
        "criticality_df =register_df.groupBy(\"station\", \"dayofweek\", \"hour\").agg(avg(\"fullstatus\").alias(\"criticality\"))\n",
        "\n",
        "# Виведемо результат\n",
        "criticality_df.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7x0WSazl3fw9",
        "outputId": "ca35550b-8cf6-4d82-868d-6642f551475e"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+--------+-----------+\n",
            "|station| dayofweek|    hour|criticality|\n",
            "+-------+----------+--------+-----------+\n",
            "|      1|2023-05-15|18:46:00|        0.0|\n",
            "|      1|2023-05-15|23:18:00|        0.0|\n",
            "|      1|2023-05-17|08:46:00|        0.0|\n",
            "|      1|2023-05-17|11:02:00|        0.0|\n",
            "|      1|2023-05-17|13:16:00|        0.0|\n",
            "|      1|2023-05-19|06:54:00|        0.0|\n",
            "|      1|2023-05-19|08:00:00|        0.0|\n",
            "|      1|2023-05-21|12:02:00|        0.0|\n",
            "|      1|2023-05-21|19:46:00|        0.0|\n",
            "|      1|2023-05-21|21:22:00|        0.0|\n",
            "|      1|2023-05-22|06:02:00|        0.0|\n",
            "|      1|2023-05-22|23:10:00|        0.0|\n",
            "|      1|2023-05-23|06:08:00|        0.0|\n",
            "|      1|2023-05-23|13:58:00|        0.0|\n",
            "|      1|2023-05-23|19:46:00|        0.0|\n",
            "|      1|2023-05-24|17:18:00|        0.0|\n",
            "|      1|2023-05-24|20:36:00|        0.0|\n",
            "|      1|2023-05-25|05:58:00|        0.0|\n",
            "|      1|2023-05-25|15:58:00|        0.0|\n",
            "|      1|2023-05-25|17:48:00|        0.0|\n",
            "+-------+----------+--------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Виберіть лише рядки з `criticality > threshold`\n",
        "\n",
        "> `threshold` є деякою бізнес-вимогою, тому візьміть випадкове число від `0.1` до `0.5`, яке вам подобається :)"
      ],
      "metadata": {
        "id": "5wJnNFHHV7Aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.25\n",
        "\n",
        "# ваш код\n",
        "# Вибираємо лише рядки, де значення критичності перевищує поріг\n",
        "selected_criticality_df = criticality_df.filter(criticality_df[\"criticality\"] > threshold)\n",
        "\n",
        "# Виводимо результат\n",
        "selected_criticality_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXUEm0lAV1r3",
        "outputId": "ad1d7b01-6590-42dd-d61e-cf2ead8aaa80"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+--------+-----------+\n",
            "|station| dayofweek|    hour|criticality|\n",
            "+-------+----------+--------+-----------+\n",
            "|      1|2023-05-26|21:02:00|        1.0|\n",
            "|      1|2023-05-26|21:52:00|        1.0|\n",
            "|      1|2023-06-13|23:32:00|        1.0|\n",
            "|      1|2023-06-21|22:00:00|        1.0|\n",
            "|      1|2023-06-25|22:56:00|        1.0|\n",
            "|      1|2023-07-02|23:22:00|        1.0|\n",
            "|      1|2023-07-03|01:50:00|        1.0|\n",
            "|      1|2023-07-07|00:42:00|        1.0|\n",
            "|      1|2023-07-07|02:00:00|        1.0|\n",
            "|      1|2023-07-08|05:42:00|        1.0|\n",
            "|      1|2023-08-03|02:32:00|        1.0|\n",
            "|      1|2023-08-05|03:04:00|        1.0|\n",
            "|      1|2023-08-07|02:06:00|        1.0|\n",
            "|      1|2023-08-11|00:52:00|        1.0|\n",
            "|      1|2023-08-18|07:16:00|        1.0|\n",
            "|      1|2023-08-20|00:40:00|        1.0|\n",
            "|      1|2023-08-20|08:58:00|        1.0|\n",
            "|      1|2023-08-24|02:38:00|        1.0|\n",
            "|      1|2023-08-24|08:00:00|        1.0|\n",
            "|      1|2023-08-24|08:18:00|        1.0|\n",
            "+-------+----------+--------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Прочитайте вміст вхідного файлу `stations.csv` і збережіть його у DataFrame.\n",
        "\n",
        "Вхідний файл має заголовок.\n",
        "\n",
        "Схема даних:\n",
        "* id: integer (nullable = true)\n",
        "* longitude: double (nullable = true)\n",
        "* latitude: double (nullable = true)\n",
        "* name: string (nullable = true)"
      ],
      "metadata": {
        "id": "iYtEbkM1V_4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Ініціалізуємо SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Read Stations CSV\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Читаємо вміст вхідного файлу stations.csv та зберігаємо його у DataFrame\n",
        "stations_df = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .option(\"delimiter\", \"\\t\") \\\n",
        "    .load(\"stations.csv\")\n",
        "\n",
        "# Виведемо перші 5 рядків для перевірки\n",
        "stations_df.show(5)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vk4wtR6oV1nQ",
        "outputId": "bbfb755c-a74e-489b-9755-cfb2c8f7c1e7"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+---------+--------------------+\n",
            "| id|longitude| latitude|                name|\n",
            "+---+---------+---------+--------------------+\n",
            "|  1| 2.180019|41.397978|Gran Via Corts Ca...|\n",
            "|  2| 2.176414|41.394381|       Plaza TetuÃ¡n|\n",
            "|  3| 2.181164| 41.39375|             Ali Bei|\n",
            "|  4|   2.1814|41.393364|               Ribes|\n",
            "|  5| 2.180214|41.391072|  Pg LluÃ­s Companys|\n",
            "+---+---------+---------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Об’єднайте (`JOIN`) вибрані критичні часові інтервали з таблицею станцій, щоб отримати координати станцій"
      ],
      "metadata": {
        "id": "BkD68LcAWES_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Объединяем DataFrame'ы по ключу \"station\"\n",
        "merged_df = selected_criticality_df.join(stations_df, selected_criticality_df.station == stations_df.id, \"inner\")\n",
        "\n",
        "# Выводим результат объединения\n",
        "merged_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViFnS6f3V1ff",
        "outputId": "cd9cc5b0-c0da-421e-bd9b-925d044ca0b7"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+--------+-----------+---+---------+---------+--------------------+\n",
            "|station| dayofweek|    hour|criticality| id|longitude| latitude|                name|\n",
            "+-------+----------+--------+-----------+---+---------+---------+--------------------+\n",
            "|      1|2023-05-26|21:02:00|        1.0|  1| 2.180019|41.397978|Gran Via Corts Ca...|\n",
            "|      1|2023-05-26|21:52:00|        1.0|  1| 2.180019|41.397978|Gran Via Corts Ca...|\n",
            "|      1|2023-06-13|23:32:00|        1.0|  1| 2.180019|41.397978|Gran Via Corts Ca...|\n",
            "|      1|2023-06-21|22:00:00|        1.0|  1| 2.180019|41.397978|Gran Via Corts Ca...|\n",
            "|      1|2023-06-25|22:56:00|        1.0|  1| 2.180019|41.397978|Gran Via Corts Ca...|\n",
            "|      1|2023-07-02|23:22:00|        1.0|  1| 2.180019|41.397978|Gran Via Corts Ca...|\n",
            "|      1|2023-07-03|01:50:00|        1.0|  1| 2.180019|41.397978|Gran Via Corts Ca...|\n",
            "|      1|2023-07-07|00:42:00|        1.0|  1| 2.180019|41.397978|Gran Via Corts Ca...|\n",
            "|      1|2023-07-07|02:00:00|        1.0|  1| 2.180019|41.397978|Gran Via Corts Ca...|\n",
            "|      1|2023-07-08|05:42:00|        1.0|  1| 2.180019|41.397978|Gran Via Corts Ca...|\n",
            "|      1|2023-08-03|02:32:00|        1.0|  1| 2.180019|41.397978|Gran Via Corts Ca...|\n",
            "|      1|2023-08-05|03:04:00|        1.0|  1| 2.180019|41.397978|Gran Via Corts Ca...|\n",
            "|      1|2023-08-07|02:06:00|        1.0|  1| 2.180019|41.397978|Gran Via Corts Ca...|\n",
            "|      1|2023-08-11|00:52:00|        1.0|  1| 2.180019|41.397978|Gran Via Corts Ca...|\n",
            "|      1|2023-08-18|07:16:00|        1.0|  1| 2.180019|41.397978|Gran Via Corts Ca...|\n",
            "|      1|2023-08-20|00:40:00|        1.0|  1| 2.180019|41.397978|Gran Via Corts Ca...|\n",
            "|      1|2023-08-20|08:58:00|        1.0|  1| 2.180019|41.397978|Gran Via Corts Ca...|\n",
            "|      1|2023-08-24|02:38:00|        1.0|  1| 2.180019|41.397978|Gran Via Corts Ca...|\n",
            "|      1|2023-08-24|08:00:00|        1.0|  1| 2.180019|41.397978|Gran Via Corts Ca...|\n",
            "|      1|2023-08-24|08:18:00|        1.0|  1| 2.180019|41.397978|Gran Via Corts Ca...|\n",
            "+-------+----------+--------+-----------+---+---------+---------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Відсортуйте вміст DataFrame"
      ],
      "metadata": {
        "id": "SY5MOQOTWFaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_drop = [\"id\", \"name\"]\n",
        "sorted_df = merged_df.drop(*columns_to_drop)\n",
        "# Выводим результат объединения\n",
        "sorted_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tl8tobF13f0m",
        "outputId": "e7ece4ef-a98a-4b1f-b802-f9d1a80c8a67"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+--------+-----------+---------+---------+\n",
            "|station| dayofweek|    hour|criticality|longitude| latitude|\n",
            "+-------+----------+--------+-----------+---------+---------+\n",
            "|      1|2023-05-26|21:02:00|        1.0| 2.180019|41.397978|\n",
            "|      1|2023-05-26|21:52:00|        1.0| 2.180019|41.397978|\n",
            "|      1|2023-06-13|23:32:00|        1.0| 2.180019|41.397978|\n",
            "|      1|2023-06-21|22:00:00|        1.0| 2.180019|41.397978|\n",
            "|      1|2023-06-25|22:56:00|        1.0| 2.180019|41.397978|\n",
            "|      1|2023-07-02|23:22:00|        1.0| 2.180019|41.397978|\n",
            "|      1|2023-07-03|01:50:00|        1.0| 2.180019|41.397978|\n",
            "|      1|2023-07-07|00:42:00|        1.0| 2.180019|41.397978|\n",
            "|      1|2023-07-07|02:00:00|        1.0| 2.180019|41.397978|\n",
            "|      1|2023-07-08|05:42:00|        1.0| 2.180019|41.397978|\n",
            "|      1|2023-08-03|02:32:00|        1.0| 2.180019|41.397978|\n",
            "|      1|2023-08-05|03:04:00|        1.0| 2.180019|41.397978|\n",
            "|      1|2023-08-07|02:06:00|        1.0| 2.180019|41.397978|\n",
            "|      1|2023-08-11|00:52:00|        1.0| 2.180019|41.397978|\n",
            "|      1|2023-08-18|07:16:00|        1.0| 2.180019|41.397978|\n",
            "|      1|2023-08-20|00:40:00|        1.0| 2.180019|41.397978|\n",
            "|      1|2023-08-20|08:58:00|        1.0| 2.180019|41.397978|\n",
            "|      1|2023-08-24|02:38:00|        1.0| 2.180019|41.397978|\n",
            "|      1|2023-08-24|08:00:00|        1.0| 2.180019|41.397978|\n",
            "|      1|2023-08-24|08:18:00|        1.0| 2.180019|41.397978|\n",
            "+-------+----------+--------+-----------+---------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write to file:"
      ],
      "metadata": {
        "id": "qQjeRCUjWIP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Запись DataFrame в CSV-файл\n",
        "sorted_df.write.csv(\"ready_file.csv\", header=True)\n"
      ],
      "metadata": {
        "id": "DcGDZEg0WHpJ"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Зупинка Spark"
      ],
      "metadata": {
        "id": "t6HK1ppvWKwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "ed6oxBzYWH__"
      },
      "execution_count": 93,
      "outputs": []
    }
  ]
}