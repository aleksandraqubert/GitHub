{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1uNWf1Y4uuOH0mbsDzl5eck-IoH7Pw4-D",
      "authorship_tag": "ABX9TyPz244jCO6ajLJoeF5aVwSH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aleksandraqubert/GitHub/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elE0Hh-FDpMK",
        "outputId": "cd10b0a5-2aff-44aa-acca-4bfa859d6631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=678d99e8b94b918405e8a294b6f6da44052feb059696c9251a011ab788dfc7fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "number_cores = 2\n",
        "memory_gb = 4\n",
        "is_full_dataset = True"
      ],
      "metadata": {
        "id": "xHc9jAfe1U0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "import os\n",
        "number_cores = 2\n",
        "memory_gb = 4\n",
        "conf = (\n",
        "    SparkConf()\n",
        "        .setAppName(\"Spark Rdd Task\")\n",
        "        .setMaster(f'local[{number_cores}]')\n",
        "        .set('spark.driver.memory', f'{memory_gb}g')\n",
        ")\n",
        "\n",
        "sc = SparkContext.getOrCreate(conf=conf)"
      ],
      "metadata": {
        "id": "lMFYTs8lCqW7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if is_full_dataset:\n",
        "    if not os.path.exists('/Reviews.csv'):\n",
        "        sc.stop()\n",
        "        raise Exception(\"\"\"\n",
        "            Download the '/Reviews.csv' file from https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews\n",
        "            and put it in 'input' folder\n",
        "        \"\"\")\n",
        "    else:\n",
        "        inputRdd = sc.textFile(\"/Reviews.csv\")\n",
        "else:\n",
        "    inputRdd = sc.textFile(\"/sample.csv\")"
      ],
      "metadata": {
        "id": "6jCnE6KcCVuk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Новый раздел"
      ],
      "metadata": {
        "id": "DAhnplFnFEvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Видалення рядку заголовку\n",
        "filteredInput = inputRdd.filter(lambda line: line.startswith(\"Id,\") == False)"
      ],
      "metadata": {
        "id": "taA2C0uqCZkz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ваш код починається тут\n",
        "# працювати треба з RDD filteredInput\n",
        "\n",
        "    # Розділяємо кожен рядок на поля\n",
        "split_fields = filteredInput.map(lambda row: row.split(','))\n",
        "\n",
        "# Створюємо пари (`UserId` та `ProductId`)\n",
        "user_product_pairs = split_fields.map(lambda own: (own[2], own[1]))\n",
        "\n",
        "# Групуємо за Користувачем та об'єднуємо всі Товари до списку унікальних продуктів\n",
        "unique_products_per_user = user_product_pairs.groupByKey().mapValues(lambda product: list(set(product)))\n",
        "unique_products_per_user.take(10)\n",
        "from itertools import combinations\n",
        "# Функція для створення пар продуктів та підрахунку їх\n",
        "def get_product_pairs(products):\n",
        "    product_pairs = list(combinations(products, 2))  # Усі можливі комбінації товарів\n",
        "    return [(pair, 1) for pair in product_pairs]\n",
        "\n",
        "# Створюємо всі пари продуктів для кожного користувача\n",
        "user_product_pairs = unique_products_per_user.flatMap(lambda user_products: get_product_pairs(user_products[1]))\n",
        "\n",
        "# Вивовдимо результат\n",
        "user_product_pairs.take(10)\n",
        "\n",
        "from itertools import combinations\n",
        "from collections import Counter\n",
        "\n",
        "# Функція для створення пар товарів та підрахунку їх кількості\n",
        "def get_product_pairs(products):\n",
        "    product_pairs = list(combinations(products, 2))  # Всі можливі комбінації товарів\n",
        "    return Counter(product_pairs)  # Підрахунок кількості кожної пари товарів\n",
        "\n",
        "# Створюємо всі пари товарів для кожного користувача та підраховуємо їх кількість\n",
        "product_pairs_count = unique_products_per_user.flatMapValues(get_product_pairs)\\\n",
        "                                               .map(lambda x: (x[1], 1))\\\n",
        "                                               .reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# Сортуємо пари продуктів за кількістю від найбільшої до найменшої\n",
        "sorted_product_pairs_count = product_pairs_count.sortBy(lambda x: x[1], ascending=False)\n",
        "\n",
        "# Беремо лише перші 10 пар продуктів та їх кількість\n",
        "top_10_product_pairs_count = sorted_product_pairs_count.take(10)\n",
        "\n",
        "# Виводимо результат\n",
        "top_10_product_pairs_count\n",
        "\n",
        "# Відкриваємо файл для запису\n",
        "with open('top_10_product_pairs_count.txt', 'w') as file:\n",
        "    # Записуємо заголовок\n",
        "    file.write(\"Топ 10 пар продуктів та їх кількість:\\n\")\n",
        "    # Записуємо кожну пару та її кількість\n",
        "    for pair, count in top_10_product_pairs_count:\n",
        "        file.write(f\"{pair}: {count}\\n\")\n"
      ],
      "metadata": {
        "id": "OHyWpSY_CZ2y"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "N5eSOpGxCZ-S"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}